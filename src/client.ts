// å¼•å…¥æ‰€éœ€æ¨¡å—
import { Client } from "@modelcontextprotocol/sdk/client/index.js" // MCP å®¢æˆ·ç«¯æ ¸å¿ƒç±»
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js" // ç”¨äºé€šè¿‡ stdio é€šä¿¡çš„ä¼ è¾“å±‚
import readline from "readline/promises" // ç”¨äºå‘½ä»¤è¡Œäº¤äº’
import dotenv from "@dotenvx/dotenvx"

import { OpenAI } from "openai" // OpenAI å®˜æ–¹ SDK
import { ChatCompletionMessageParam, type ChatCompletionTool } from "openai/resources/chat/completions" // èŠå¤©æ¶ˆæ¯ç±»å‹

// åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config()

// è¯»å–ç¯å¢ƒå˜é‡
const OPENAI_API_KEY = process.env.OPENAI_API_KEY!
const BASE_URL = process.env.BASE_URL || undefined
const MODEL = process.env.MODEL || "Qwen/QwQ-32B"

// å¦‚æœæœªè®¾ç½® OpenAI API Keyï¼Œåˆ™æŠ¥é”™
if (!OPENAI_API_KEY) {
  throw new Error("âŒ è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")
}

// MCP + OpenAI å®¢æˆ·ç«¯ç±»
class MCPClient {
  mcp: Client // MCP å®¢æˆ·ç«¯å®ä¾‹
  openai: OpenAI // OpenAI å®¢æˆ·ç«¯å®ä¾‹
  tools: Array<{ name: string; description: string; input_schema: any }> = [] // å·¥å…·åˆ—è¡¨

  constructor() {
    // åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
    this.mcp = new Client({ name: "mcp-client-openai", version: "1.0.0" })

    // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    this.openai = new OpenAI({
      apiKey: OPENAI_API_KEY,
      baseURL: BASE_URL,
    })
  }

  // è¿æ¥ MCP æœåŠ¡å™¨ï¼ˆæ”¯æŒ .js æˆ– .pyï¼‰
  async connectToServer(serverScriptPath: string) {
    const isJs = serverScriptPath.endsWith(".js")
    const isPy = serverScriptPath.endsWith(".py")

    if (!isJs && !isPy) throw new Error("âŒ æœåŠ¡å™¨è„šæœ¬å¿…é¡»æ˜¯ .js æˆ– .py æ–‡ä»¶")

    // åˆ¤æ–­å¹³å°ï¼Œé€‰æ‹©å¯åŠ¨å‘½ä»¤ï¼ˆWindows ç”¨ pythonï¼ŒUnix ç”¨ python3ï¼‰
    const command = isPy ? (process.platform === "win32" ? "python" : "python3") : process.execPath // å¦‚æœæ˜¯ JS è„šæœ¬ï¼Œåˆ™ä½¿ç”¨ Node è¿è¡Œ

    // å¯åŠ¨æœåŠ¡å™¨è„šæœ¬ï¼Œå¹¶å»ºç«‹ stdio é€šä¿¡
    const transport = new StdioClientTransport({
      command,
      args: [serverScriptPath],
    })

    // ä½¿ç”¨ MCP å®¢æˆ·ç«¯è¿æ¥æœåŠ¡å™¨
    this.mcp.connect(transport)

    // è·å–æœåŠ¡å™¨æš´éœ²çš„å·¥å…·ä¿¡æ¯
    const toolsResult = await this.mcp.listTools()

    this.tools = toolsResult.tools.map((tool: any) => ({
      name: tool.name,
      description: tool.description,
      input_schema: tool.inputSchema,
    }))

    const tools = this.tools.map((t) => t.name)
    console.log("âœ… å·²è¿æ¥æœåŠ¡å™¨ï¼Œæ”¯æŒå·¥å…·ï¼š", tools)
  }

  // å¯åŠ¨å‘½ä»¤è¡Œå¯¹è¯å¾ªç¯
  async chatLoop() {
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    })

    console.log("âœ… MCP Client å¯åŠ¨å®Œæˆ")
    console.log("ğŸ’¬ è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæˆ–è¾“å…¥ 'quit' é€€å‡º")

    while (true) {
      const input = await rl.question("\nQuery: ")
      if (input.toLowerCase() === "quit") break

      const result = await this.processQuery(input)
      console.log("\nğŸ§  å›å¤ï¼š\n" + result)
    }

    rl.close()
  }

  // å…³é—­ MCP è¿æ¥
  async cleanup() {
    await this.mcp.close()
  }

  // å¤„ç†ç”¨æˆ·æé—®
  async processQuery(query: string): Promise<string> {
    const messages: ChatCompletionMessageParam[] = [
      { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚" },
      { role: "user", content: query },
    ]

    // æ„é€  tools æ•°ç»„ï¼Œä¾› OpenAI è°ƒç”¨
    const tools = this.tools.map<ChatCompletionTool>((tool) => ({
      type: "function",
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.input_schema,
        strict: false, // Qwen ç‰¹æœ‰å­—æ®µï¼ŒOpenAI å¯å¿½ç•¥
      },
    }))

    try {
      // ç¬¬ä¸€æ¬¡è¯·æ±‚ OpenAIï¼Œçœ‹æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
      const response = await this.openai.chat.completions.create({
        model: MODEL,
        messages,
        tools,
        tool_choice: "auto", // è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
        max_tokens: 1000,
        temperature: 0.7,
      })

      const message = response.choices[0].message

      // å¦‚æœæ¨¡å‹è¿”å›äº†å·¥å…·è°ƒç”¨è¯·æ±‚
      if (message.tool_calls?.length) {
        const toolCall = message.tool_calls[0]
        const toolName = toolCall.function.name
        const toolArgs = JSON.parse(toolCall.function.arguments)

        console.log(`\nğŸ”§ è°ƒç”¨å·¥å…·ï¼š${toolName}`)
        console.log(`ğŸ“¦ å‚æ•°ï¼š${JSON.stringify(toolArgs)}`)

        // ä½¿ç”¨ MCP è°ƒç”¨æœ¬åœ°å·¥å…·
        const toolResult: any = await this.mcp.callTool({
          name: toolName,
          arguments: toolArgs,
        })

        // è·å–å·¥å…·çš„æ–‡æœ¬è¿”å›ç»“æœ
        const resultText = toolResult?.content?.[0]?.text ?? "[å·¥å…·æ— ç»“æœè¿”å›]"

        // æŠŠè°ƒç”¨è¿‡ç¨‹åŠ å…¥å¯¹è¯ä¸Šä¸‹æ–‡
        messages.push({
          role: "assistant",
          content: null,
          tool_calls: [toolCall],
        })

        messages.push({
          role: "tool",
          tool_call_id: toolCall.id,
          content: resultText,
        })

        // å†æ¬¡å‘ OpenAI å‘é€è¯·æ±‚ï¼Œè·å–åŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›å¤
        const finalResponse = await this.openai.chat.completions.create({
          model: MODEL,
          messages,
          max_tokens: 1000,
        })

        return finalResponse.choices[0].message?.content || "[æ— è¿”å›å†…å®¹]"
      }

      // å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›æ¨¡å‹å›å¤
      return message?.content || "[æ— è¿”å›å†…å®¹]"
    } catch (err: any) {
      return `âŒ OpenAI è¯·æ±‚å‡ºé”™: ${err.message}`
    }
  }
}

// ä¸»å‡½æ•°å…¥å£
async function main() {
  // æ£€æŸ¥æ˜¯å¦æä¾›äº†æœåŠ¡å™¨è„šæœ¬è·¯å¾„
  if (process.argv.length < 3) return console.log("ç”¨æ³•: node dist/index.js <path_to_server_script>")

  const mcpClient = new MCPClient()

  try {
    // è¿æ¥ MCP å·¥å…·æœåŠ¡å™¨
    await mcpClient.connectToServer(process.argv[2])
    // å¯åŠ¨äº¤äº’å¾ªç¯
    await mcpClient.chatLoop()
  } finally {
    // æ¸…ç†å¹¶é€€å‡º
    await mcpClient.cleanup()
    process.exit(0)
  }
}

// å¯åŠ¨ç¨‹åº
main()
